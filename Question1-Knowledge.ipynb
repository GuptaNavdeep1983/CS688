{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZFNqKAxEj0OZxLcydLxtr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS688/blob/main/Question1-Knowledge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQNadAd8n3Lw",
        "outputId": "de626ac3-82fa-43ea-eca4-a327d49172d3"
      },
      "source": [
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from pprint import pprint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGKeRpRioFym"
      },
      "source": [
        "df = pd.read_csv(\"pubmed_results.csv\")\n",
        "df.dropna(inplace=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7a7ZweQ8Knf"
      },
      "source": [
        "def extract_words(text):\n",
        "    temp = text.split() # Split the text on whitespace\n",
        "    text_words = []\n",
        "\n",
        "    for word in temp:\n",
        "        # Remove any punctuation characters present in the beginning of the word\n",
        "        while word[0] in string.punctuation:\n",
        "            word = word[1:]\n",
        "\n",
        "        # Remove any punctuation characters present in the end of the word\n",
        "        while word[-1] in string.punctuation:\n",
        "            word = word[:-1]\n",
        "\n",
        "        # Append this word into our list of words.\n",
        "        text_words.append(word.lower())\n",
        "        \n",
        "    return text_words"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHXdTDF6oRka"
      },
      "source": [
        "all_documents = []\n",
        "all_sentences = df['title'].to_numpy()\n",
        "\n",
        "# for line in all_sentences:\n",
        "#   all_words.extend(line.strip().split())\n",
        "all_documents = [nltk.word_tokenize(sent) for sent in all_sentences]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsnEYpYuoNdq"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "for i in range(len(all_words)):\n",
        "    all_documents[i] = [w for w in all_documents[i] if w not in stopwords.words('english')]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYSojSpCtIAo"
      },
      "source": [
        "def tf_idf(corpus_words):\n",
        "    IDF = inv_doc_freq(corpus_words)\n",
        "    \n",
        "    TFIDF = []\n",
        "    \n",
        "    for document in corpus_words:\n",
        "        TFIDF.append(Counter(document))\n",
        "    \n",
        "    for document in TFIDF:\n",
        "        for word in document:\n",
        "            document[word] = document[word]*IDF[word]\n",
        "            \n",
        "    return TFIDF\n",
        "\n",
        "def inv_doc_freq(corpus_words):\n",
        "    number_docs = len(corpus_words)\n",
        "    \n",
        "    document_count = {}\n",
        "\n",
        "    for document in corpus_words:\n",
        "        word_set = set(document)\n",
        "\n",
        "        for word in word_set:\n",
        "            document_count[word] = document_count.get(word, 0) + 1\n",
        "    \n",
        "    IDF = {}\n",
        "    \n",
        "    for word in document_count:\n",
        "        IDF[word] = np.log(number_docs/document_count[word])\n",
        "        \n",
        "    \n",
        "    return IDF\n",
        "\n",
        "def term_document_matrix(TFIDF, word_list, word_dict):\n",
        "    vocabulary_size = len(word_dict)\n",
        "    number_documents = len(TFIDF)\n",
        "    \n",
        "    TDM = np.zeros((vocabulary_size, number_documents))\n",
        "    \n",
        "    for doc in range(number_documents):\n",
        "        document = TFIDF[doc]\n",
        "        \n",
        "        for word in document.keys():\n",
        "            pos = word_dict[word]\n",
        "            \n",
        "            TDM[pos, doc] = document[word]\n",
        "            \n",
        "    return TDM\n",
        "\n",
        "def build_vocabulary(TFIDF):\n",
        "    words = set()\n",
        "    \n",
        "    for document in TFIDF:\n",
        "        words |= document.keys()\n",
        "    \n",
        "    word_list = list(words)\n",
        "    word_dict = dict(zip(word_list, range(len(word_list))))\n",
        "    \n",
        "    return word_dict, word_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNV03H4Y8bCN"
      },
      "source": [
        "IDF = inv_doc_freq(all_documents)\n",
        "\n",
        "pprint(IDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRP7XQNZHxCM"
      },
      "source": [
        "IDF\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb0hC-eJ8xn9"
      },
      "source": [
        "TFIDF = tf_idf(all_documents)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdUsmtV8_Zs8"
      },
      "source": [
        "word_dict, word_list = build_vocabulary(TFIDF)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU7QrOll-mW-"
      },
      "source": [
        "TDM = term_document_matrix(TFIDF, word_list, word_dict)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHCicxWTJQ5V",
        "outputId": "c3c60dbf-02c2-41d9-80dc-690a78ca7887"
      },
      "source": [
        "TDM.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4946, 808)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW6k1X2e91AQ"
      },
      "source": [
        "def find_related_docs(tweet, TDM):\n",
        "    new_vector = np.zeros(TDM.shape[1])\n",
        "    \n",
        "    for word in tweet:\n",
        "        if word in word_dict:\n",
        "          pos = word_dict[word]\n",
        "          new_vector += TDM[pos, :]\n",
        "        \n",
        "    # Now the entries of new_vector tell us which documents are activated by this one.\n",
        "    # Let's extract the list of documents sorted by activation\n",
        "    doc_list = sorted(zip(range(TDM.shape[1]), new_vector), key=lambda x:x[1], reverse=True)\n",
        "    \n",
        "    return doc_list"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFHz-nJZ9_0d"
      },
      "source": [
        "research_words = [\"Obesity\", \"Cancer\", \"Covid-19\", \"wearable\", \"mental health\", \"influenza\"]\n",
        "related = find_related_docs(research_words, TDM)\n",
        "related"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDppvzR_64t",
        "outputId": "5855c2c7-4277-4462-ed93-e440ab5e9f8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for tweet, score in related[:14]:\n",
        "    print(tweet,score, \" \".join(all_documents[tweet]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "167 6.694562058521095 Clinical Specificities Obesity Care : The Transformations Dissolution 'Will ' 'Drives ' .\n",
            "803 6.694562058521095 Using Whatsapp Consultation Covid-19 Suspected Patients Pandemic Polyclinics ; A Retrospective Case Control Study .\n",
            "402 6.0014148779611505 Enhanced virulence clade 2.3.2.1 highly pathogenic avian influenza A H5N1 viruses ferrets .\n",
            "567 6.0014148779611505 Impact variations potential glycosylation sites hemagglutinin H9N2 influenza virus .\n",
            "340 4.497337481184876 Single-Site Robotic Radical Hysterectomy Sentinel Lymphnode Biopsy Cervical Cancer : A Case Report .\n",
            "546 4.497337481184876 Lung cancer smoking trends young Switzerland : study based data National Institute Cancer Epidemiology Registration Swiss Health Surveys .\n",
            "573 4.497337481184876 Staging prognosis oropharyngeal carcinoma according 8th Edition American Joint Committee Cancer Staging Manual human papillomavirus infection .\n",
            "589 4.497337481184876 lincRNA-p21 Mediates Anti-Cancer Effect Ginkgo Biloba Extract EGb 761 Stabilizing E-Cadherin Protein Colon Cancer .\n",
            "644 4.497337481184876 [ The interpretation Management Guidelines Children Thyroid Nodules Differentiated Thyroid Cancer ] .\n",
            "675 4.497337481184876 Increased Progastrin-Releasing Peptide Expression Associated Progression Gastric Cancer Patients .\n",
            "719 4.497337481184876 Using Ovine Extracellular Matrix Difficult Close Excisions Common Skin Cancer : Evolving New Technique .\n",
            "760 4.497337481184876 [ shRNA-Mediated Suppression γ-Synuclein Leading Downregulation p38/ERK/JNK Phosphorylation Cell Cycle Arrest Endometrial Cancer Cells ] .\n",
            "762 4.497337481184876 [ Novel miRNAs Potential Regulators PD-1/PD-L1 Immune Checkpoint , Prognostic Value MIR9-1 MIR124-2 Methylation Ovarian Cancer ] .\n",
            "0 0.0 Golimumab treatment psoriatic arthritis .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "6777zVlbRvZR",
        "outputId": "b439ac4d-b71f-4d56-8f2c-91a83ee6bd51"
      },
      "source": [
        "\n",
        " \n",
        "rake_object = RAKE.rake('SmartStoplist.txt', 5, 3, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-a4982bba43a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrake_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAKE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SmartStoplist.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    }
  ]
}