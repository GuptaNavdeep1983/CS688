{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0CJ8h0tchcFYivEn8KSFv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS688/blob/main/Assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Jw5OkD-WJl"
      },
      "source": [
        "References:\n",
        "\n",
        "https://datascience.stackexchange.com/questions/54904/how-to-avoid-tokenizing-w-sklearn-feature-extraction\n",
        "\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
        "\n",
        "https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform\n",
        "\n",
        "https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "939EWQDE89Pc"
      },
      "source": [
        "pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-_jecXwgU3_"
      },
      "source": [
        "import statsmodels.api as sm \n",
        "import pylab as py \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import datetime\n",
        "from zipfile import ZipFile\n",
        "from gzip import decompress\n",
        "from json import loads\n",
        "from requests import get\n",
        "import requests, zipfile, io\n",
        "from bs4 import BeautifulSoup\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel, LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_punctuation2, strip_short, strip_numeric, stem_text\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *"
      ],
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RikVoLscGp22",
        "outputId": "2bdf1e7d-addd-4455-eeb9-ad7e1f38e113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjTPViKKgafF"
      },
      "source": [
        "def get_crowd_funding_data_links(filter_by_years):\n",
        "    # get the content of the website\n",
        "    content = urllib.request.urlopen('https://webrobots.io/indiegogo-dataset/').read()\n",
        "    soup = BeautifulSoup(content,features='html.parser')\n",
        "    \n",
        "    all_stories = pd.DataFrame([], columns=['Year', 'Month', 'json', 'csv'])\n",
        "    parent_div = soup.find(name='div', attrs={'class':'fusion-text'})\n",
        "\n",
        "    # Iterate through the divs to find the list of Month-Year wise anchor tags\n",
        "    index = 0\n",
        "    for year_data in parent_div.find_all(name='ul'):\n",
        "        all_months = year_data.find_all('li')\n",
        "        for month in all_months:\n",
        "            complete_text = month.text\n",
        "            date_time_obj = datetime.datetime.strptime((complete_text.split('[')[0]).strip(), '%Y-%m-%d')\n",
        "            if date_time_obj.year in filter_by_years:\n",
        "                all_stories.loc[index, \"Year\"] = date_time_obj.year\n",
        "                all_stories.loc[index, \"Month\"] = str(date_time_obj.month) if date_time_obj.month >=10 else f'0{date_time_obj.month}'\n",
        "                all_stories.loc[index, \"json\"] = month.find_all('a')[0][\"href\"]\n",
        "                all_stories.loc[index, \"csv\"] = month.find_all('a')[1][\"href\"]\n",
        "                index = index + 1\n",
        "    return all_stories\n",
        "\n",
        "def get_current_index_data(df, indx, columns):\n",
        "    r = requests.get(df[\"csv\"][indx])\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    csv_data = z.open(\"Indiegogo.csv\")\n",
        "    data_df = pd.read_csv(csv_data, usecols=columns, dtype={'title':str, 'tagline':str})\n",
        "    return data_df\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return result"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhs2TRecglUJ"
      },
      "source": [
        "df = get_crowd_funding_data_links([2020])\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "documents = []\n",
        "for indx in df.head(5).index:\n",
        "    current_month_data = get_current_index_data(df, indx, [\"title\", \"tagline\"])\n",
        "    current_month_data.dropna(axis=0, inplace=True)\n",
        "    for index, row in current_month_data.iterrows():\n",
        "      # all_content = strip_numeric(strip_short(strip_punctuation2(strip_punctuation(remove_stopwords(row['tagline']))), minsize=4))\n",
        "      all_content = preprocess(row['tagline'])\n",
        "      documents.append(all_content)"
      ],
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMp6uX7KHA6T"
      },
      "source": [
        "# print(common_texts)\n",
        "common_dictionary = corpora.Dictionary(documents)\n",
        "common_dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQSatdh57RcW"
      },
      "source": [
        "df = pd.DataFrame(documents, columns=['content'])\n",
        "vectorizer = CountVectorizer(input='content',stop_words='english', analyzer='word', token_pattern=r'\\b[a-zA-Z]{2,}\\b',max_features=100)\n",
        "doc_term_matrix = vectorizer.fit_transform(df['content'])\n",
        "# doc_term_matrix = vectorizer.transform(df['content'])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "df_dictionary = pd.DataFrame(feature_names)\n",
        "dictionary = df_dictionary.to_dict()[0]\n",
        "# df_doc_term_matrix = pd.DataFrame(doc_term_matrix.toarray())\n",
        "term_doc_matrix = doc_term_matrix.transpose()"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAG0J-ufyJyO",
        "outputId": "d73bcef8-77ac-4416-8b2c-9293d1af9e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "common_corpus = [common_dictionary.doc2bow(doc) for doc in documents]\n",
        "print(len(common_corpus))"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "164361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBwPMD7G8HOK"
      },
      "source": [
        "def create_gensim_lsi_model(common_corpus, common_dictionary):\n",
        "    # generate LSI model\n",
        "    lsamodel = LsiModel(common_corpus, id2word = common_dictionary, num_topics=5)  # train model\n",
        "    print(lsamodel.print_topics())\n",
        "    return lsamodel"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iG6F3iPuhpL"
      },
      "source": [
        "def create_gensim_lda_model(common_corpus, common_dictionary):\n",
        "    # generate LDA model\n",
        "    lsamodel = gensim.models.LdaMulticore(common_corpus, num_topics=5, id2word=common_dictionary, passes=10, workers=2)  # train model\n",
        "    print(lsamodel.print_topics())\n",
        "    return lsamodel"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlogTBQUJm14",
        "outputId": "9958bcaa-889a-4541-f7ea-64b428e5a3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lsi_model = create_gensim_lsi_model(common_corpus=common_corpus, common_dictionary=common_dictionary)\n"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.927*\"The\" + 0.225*\"Help\" + 0.113*\"new\" + 0.101*\"world\" + 0.078*\"help\" + 0.046*\"World\" + 0.044*\"need\" + 0.040*\"game\" + 0.038*\"book\" + 0.035*\"First\"'), (1, '0.891*\"Help\" + -0.299*\"The\" + 0.180*\"new\" + 0.112*\"help\" + 0.085*\"fund\" + 0.074*\"bring\" + 0.056*\"raise\" + 0.053*\"create\" + 0.052*\"need\" + 0.050*\"build\"'), (2, '0.682*\"help\" + 0.504*\"new\" + -0.311*\"Help\" + 0.255*\"need\" + -0.126*\"The\" + 0.097*\"world\" + 0.076*\"Please\" + 0.067*\"needs\" + 0.066*\"book\" + 0.063*\"people\"'), (3, '-0.785*\"new\" + 0.551*\"help\" + 0.196*\"need\" + 0.081*\"Help\" + 0.057*\"Please\" + -0.054*\"game\" + -0.044*\"way\" + 0.038*\"people\" + -0.038*\"album\" + -0.034*\"series\"'), (4, '0.894*\"world\" + 0.186*\"game\" + 0.155*\"book\" + -0.151*\"new\" + -0.147*\"help\" + -0.106*\"The\" + 0.095*\"people\" + -0.065*\"Help\" + -0.049*\"need\" + 0.049*\"way\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NYfambNu7XF",
        "outputId": "2ba35235-3eee-4f11-e05c-42e6ccecd5bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lda_model = create_gensim_lda_model(common_corpus, common_dictionary)"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.025*\"support\" + 0.023*\"fund\" + 0.017*\"rais\" + 0.015*\"communiti\" + 0.013*\"need\" + 0.012*\"love\" + 0.011*\"film\" + 0.010*\"album\" + 0.009*\"music\" + 0.008*\"artist\"'), (1, '0.021*\"power\" + 0.012*\"design\" + 0.012*\"smart\" + 0.011*\"light\" + 0.010*\"devic\" + 0.010*\"bike\" + 0.009*\"world\" + 0.009*\"charg\" + 0.009*\"portabl\" + 0.009*\"build\"'), (2, '0.021*\"book\" + 0.014*\"stori\" + 0.014*\"life\" + 0.012*\"world\" + 0.011*\"bring\" + 0.010*\"live\" + 0.010*\"photographi\" + 0.009*\"chang\" + 0.008*\"seri\" + 0.007*\"comic\"'), (3, '0.012*\"camera\" + 0.011*\"world\" + 0.010*\"creat\" + 0.009*\"better\" + 0.008*\"organ\" + 0.008*\"food\" + 0.008*\"free\" + 0.008*\"time\" + 0.008*\"design\" + 0.007*\"need\"'), (4, '0.047*\"game\" + 0.019*\"play\" + 0.014*\"world\" + 0.012*\"card\" + 0.010*\"adventur\" + 0.007*\"board\" + 0.006*\"action\" + 0.006*\"drive\" + 0.006*\"player\" + 0.006*\"design\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhUsvKwzrRj9"
      },
      "source": [
        "lsi_model.show_topics()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyhUmWhi4VYc"
      },
      "source": [
        "topics = lda_model.show_topics()\n",
        "for topic in topics:\n",
        "  terms = lda_model.get_topic_terms(topic[0])\n",
        "  print(f\"Topic:{topic[0]} Terms,\")\n",
        "  for term in terms:\n",
        "    print(common_dictionary[term[0]])\n",
        "# for tuple in out:\n",
        "\n",
        "# lda_model.print_topics()\n",
        "# lda_model.top_topics(corpus=common_corpus, dictionary=common_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YeQKOSIIMf7",
        "outputId": "d2eb70c2-31ff-4f88-b8fd-ab0375d54e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.025*\"support\" + 0.023*\"fund\" + 0.017*\"rais\" + 0.015*\"communiti\" + 0.013*\"need\" + 0.012*\"love\" + 0.011*\"film\" + 0.010*\"album\" + 0.009*\"music\" + 0.008*\"artist\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.021*\"power\" + 0.012*\"design\" + 0.012*\"smart\" + 0.011*\"light\" + 0.010*\"devic\" + 0.010*\"bike\" + 0.009*\"world\" + 0.009*\"charg\" + 0.009*\"portabl\" + 0.009*\"build\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.021*\"book\" + 0.014*\"stori\" + 0.014*\"life\" + 0.012*\"world\" + 0.011*\"bring\" + 0.010*\"live\" + 0.010*\"photographi\" + 0.009*\"chang\" + 0.008*\"seri\" + 0.007*\"comic\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.012*\"camera\" + 0.011*\"world\" + 0.010*\"creat\" + 0.009*\"better\" + 0.008*\"organ\" + 0.008*\"food\" + 0.008*\"free\" + 0.008*\"time\" + 0.008*\"design\" + 0.007*\"need\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.047*\"game\" + 0.019*\"play\" + 0.014*\"world\" + 0.012*\"card\" + 0.010*\"adventur\" + 0.007*\"board\" + 0.006*\"action\" + 0.006*\"drive\" + 0.006*\"player\" + 0.006*\"design\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeaEvXjYNr0l"
      },
      "source": [
        "from scipy.cluster import hierarchy"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}