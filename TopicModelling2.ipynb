{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiXCftFR6u1sJwyKocvQ0w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuptaNavdeep1983/CS688/blob/main/TopicModelling2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "PB6XRc0URNNe",
        "outputId": "c65975e9-7402-4b8c-d815-ddb6bfc9ae6c"
      },
      "source": [
        "pip install python-rake"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-rake\n",
            "  Downloading https://files.pythonhosted.org/packages/82/ec/7bd2a30264f0ec38a8a8f857fc4e6c73402c623fff3b9061fe4e790ad5e7/python_rake-1.5.0-py3-none-any.whl\n",
            "Installing collected packages: python-rake\n",
            "Successfully installed python-rake-1.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "RAKE"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQNadAd8n3Lw",
        "outputId": "336384a0-bfa2-4967-8391-7a5b204529b1"
      },
      "source": [
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "import RAKE\n",
        "import operator "
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGKeRpRioFym"
      },
      "source": [
        "df = pd.read_csv(\"pubmed_results.csv\")\n",
        "df.dropna(inplace=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhGSYb3lP4-w",
        "outputId": "bf2320b2-130c-4afc-974f-a7836d4f287c"
      },
      "source": [
        "!git clone https://github.com/zelandiya/RAKE-tutorial RAKE\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'RAKE'...\n",
            "remote: Enumerating objects: 303, done.\u001b[K\n",
            "remote: Total 303 (delta 0), reused 0 (delta 0), pack-reused 303\u001b[K\n",
            "Receiving objects: 100% (303/303), 4.31 MiB | 19.78 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7a7ZweQ8Knf"
      },
      "source": [
        "def extract_words(text):\n",
        "    temp = text.split() # Split the text on whitespace\n",
        "    text_words = []\n",
        "\n",
        "    for word in temp:\n",
        "        # Remove any punctuation characters present in the beginning of the word\n",
        "        while word[0] in string.punctuation:\n",
        "            word = word[1:]\n",
        "\n",
        "        # Remove any punctuation characters present in the end of the word\n",
        "        while word[-1] in string.punctuation:\n",
        "            word = word[:-1]\n",
        "\n",
        "        # Append this word into our list of words.\n",
        "        text_words.append(word.lower())\n",
        "        \n",
        "    return text_words"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHXdTDF6oRka"
      },
      "source": [
        "all_documents = []\n",
        "all_sentences = df['title'].to_numpy()\n",
        "\n",
        "# for line in all_sentences:\n",
        "#   all_words.extend(line.strip().split())\n",
        "all_documents = [nltk.word_tokenize(sent) for sent in all_sentences]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsnEYpYuoNdq"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "for i in range(len(all_words)):\n",
        "    all_documents[i] = [w for w in all_documents[i] if w not in stopwords.words('english')]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYSojSpCtIAo"
      },
      "source": [
        "def tf_idf(corpus_words):\n",
        "    IDF = inv_doc_freq(corpus_words)\n",
        "    \n",
        "    TFIDF = []\n",
        "    \n",
        "    for document in corpus_words:\n",
        "        TFIDF.append(Counter(document))\n",
        "    \n",
        "    for document in TFIDF:\n",
        "        for word in document:\n",
        "            document[word] = document[word]*IDF[word]\n",
        "            \n",
        "    return TFIDF\n",
        "\n",
        "def inv_doc_freq(corpus_words):\n",
        "    number_docs = len(corpus_words)\n",
        "    \n",
        "    document_count = {}\n",
        "\n",
        "    for document in corpus_words:\n",
        "        word_set = set(document)\n",
        "\n",
        "        for word in word_set:\n",
        "            document_count[word] = document_count.get(word, 0) + 1\n",
        "    \n",
        "    IDF = {}\n",
        "    \n",
        "    for word in document_count:\n",
        "        IDF[word] = np.log(number_docs/document_count[word])\n",
        "        \n",
        "    \n",
        "    return IDF\n",
        "\n",
        "def term_document_matrix(TFIDF, word_list, word_dict):\n",
        "    vocabulary_size = len(word_dict)\n",
        "    number_documents = len(TFIDF)\n",
        "    \n",
        "    TDM = np.zeros((vocabulary_size, number_documents))\n",
        "    \n",
        "    for doc in range(number_documents):\n",
        "        document = TFIDF[doc]\n",
        "        \n",
        "        for word in document.keys():\n",
        "            pos = word_dict[word]\n",
        "            \n",
        "            TDM[pos, doc] = document[word]\n",
        "            \n",
        "    return TDM\n",
        "\n",
        "def build_vocabulary(TFIDF):\n",
        "    words = set()\n",
        "    \n",
        "    for document in TFIDF:\n",
        "        words |= document.keys()\n",
        "    \n",
        "    word_list = list(words)\n",
        "    word_dict = dict(zip(word_list, range(len(word_list))))\n",
        "    \n",
        "    return word_dict, word_list"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNV03H4Y8bCN"
      },
      "source": [
        "IDF = inv_doc_freq(all_words)\n",
        "\n",
        "pprint(IDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRP7XQNZHxCM"
      },
      "source": [
        "IDF\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb0hC-eJ8xn9"
      },
      "source": [
        "TFIDF = tf_idf(all_words)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdUsmtV8_Zs8"
      },
      "source": [
        "word_dict, word_list = build_vocabulary(TFIDF)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU7QrOll-mW-"
      },
      "source": [
        "TDM = term_document_matrix(TFIDF, word_list, word_dict)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHCicxWTJQ5V",
        "outputId": "f9a771f8-0803-440f-f5d9-221aff0c6a1b"
      },
      "source": [
        "TDM"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW6k1X2e91AQ"
      },
      "source": [
        "def find_related_docs(tweet, TDM):\n",
        "    new_vector = np.zeros(TDM.shape[1])\n",
        "    \n",
        "    for word in tweet:\n",
        "        if word in word_dict:\n",
        "          pos = word_dict[word]\n",
        "          new_vector += TDM[pos, :]\n",
        "        \n",
        "    # Now the entries of new_vector tell us which documents are activated by this one.\n",
        "    # Let's extract the list of documents sorted by activation\n",
        "    doc_list = sorted(zip(range(TDM.shape[1]), new_vector), key=lambda x:x[1], reverse=True)\n",
        "    \n",
        "    return doc_list"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFHz-nJZ9_0d"
      },
      "source": [
        "research_words = [\"Obesity\", \"Cancer\", \"Covid-19\", \"wearable\", \"mental health\", \"influenza\"]\n",
        "related = find_related_docs(research_words, TDM)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDppvzR_64t"
      },
      "source": [
        "for tweet, score in related[:100]:\n",
        "    print(tweet, \" \".join(all_documents[tweet]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "6777zVlbRvZR",
        "outputId": "b439ac4d-b71f-4d56-8f2c-91a83ee6bd51"
      },
      "source": [
        "\n",
        " \n",
        "rake_object = RAKE.rake('SmartStoplist.txt', 5, 3, 4)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-a4982bba43a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrake_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAKE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SmartStoplist.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    }
  ]
}